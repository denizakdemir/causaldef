% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/theorem_bounds.R
\name{policy_regret_bound_vc}
\alias{policy_regret_bound_vc}
\title{Policy Regret Bound with VC Term (Result 3)}
\usage{
policy_regret_bound_vc(
  deficiency,
  vc_dim,
  n,
  xi = 0.05,
  utility_range = c(0, 1),
  obs_regret = NULL,
  delta_mode = c("point", "upper"),
  C = 2
)
}
\arguments{
\item{deficiency}{A \code{deficiency} object or numeric \eqn{\delta}.}

\item{vc_dim}{VC dimension of the policy class.}

\item{n}{Sample size used to learn the policy.}

\item{xi}{Failure probability (\eqn{\xi}).}

\item{utility_range}{Numeric vector c(min, max) of utility bounds.}

\item{obs_regret}{Optional observed regret under the observational regime.}

\item{delta_mode}{Passed through to \code{policy_regret_bound()} when \code{deficiency} is an object.}

\item{C}{Universal constant in the bound (default 2).}
}
\value{
An object of class \code{policy_bound} with additional field \code{complexity_penalty}.
}
\description{
Extends \code{policy_regret_bound()} with the explicit VC-complexity penalty from
Result 3: \eqn{C M \sqrt{(VC(\Pi)\log n + \log(1/\xi))/n}}.
}
